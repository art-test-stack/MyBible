# MyBible

This repository contains a non-exhaustive curated list of important research papers in the fields of Large Language Models (LLMs), Time-Series Transformers, Diffusion Models, Energy-Based Models (EB-Models), and Tsetlin Machines. The goal is to provide a comprehensive resource for researchers and practitioners interested in these areas.

## LLMs Basics

| Title |  Author(s) | Journal | Year | DOI  |  
|-------|------------|---------|------|------|
| Attention is all you need      | Vaswani et al.    | arXiv  | 2017 | [1706.03762] |
| BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | Devlin et al. | arXiv | 2018 | [1810.04805] |
| Language models are unsupervised multitask learners | Radford et al. | OpenAI | 2019 | [unsupervised-multitask] |
| Language Models are Few-Shot Learners | Brown et al. | arXiv | 2020 | [2005.14165] |
| Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention | Katharopoulos et al. | arXiv | 2020 | [2006.16236] |
| Efficient Transformers: A Survey  | Tay et al. |arXiv | 2020 | [2009.06732] |
| Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity | Fedus et al. | ICML | 2022 | [2101.03961] |
| RoFormer: Enhanced Transformer with Rotary Position Embedding | Su et al. | arXiv | 2021 | [2104.09864] |
| LoRA: Low-Rank Adaptation of Large Language Models | Hu et al. | ICLR | 2021 | [2106.09685] |
| Training Compute-Optimal Large Language Models | Hoffmann et al. | arXiv | 2022 | [2203.15556] |
| PaLM: Scaling Language Modeling with Pathways | Chowdhery et al. | arXiv | 2022 | [2204.02311] |
| FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness | Dao et al. | NeurIPS | 2022 | [2205.14135] |
| QLoRA: Efficient Finetuning of Quantized LLMs | Dettmers et al. | arXiv | 2023 | [2305.14314] |
| FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning | Dao | arXiv | 2023 | [2307.08691] |
| YaRN: Efficient Context Window Extension of Large Language Models | Peng et al. | arXiv | 2023 | [2309.00071] |
| Effective Long-Context Scaling of Foundation Models | Xiong et al. | arXiv | 2023 | [2309.16039] |
| Mistral 7B | Jiang et al. | arXiv | 2023 | [2310.06825] |
| Mamba: Linear-Time Sequence Modeling with Selective State Spaces | Gu and Dao | NeurIPS | 2023 | [2312.00752] |
| How to Train Long-Context Language Models (Effectively) | Gao et al. | arXiv | 2024 | [2410.02660] |
| The Zamba2 Suite: Technical Report | Glorion et al. | arXiv | 2024 | [2411.15242] |
| Muon is Scalable for LLM Training | Liu et al. | 2025 | arXiv | [2502.16982] |
| KIMI K2: OPEN AGENTIC INTELLIGENCE | Kimi Team | arXiv | 2025 | [2507.20534] |

## LLM Datasets

| Title |  Author(s) | Journal | Year | DOI  |  
|-------|------------|---------|------|------|
| SQUAD: 100,000+ Questions for Machine Comprehension of Text | Rajpurkar et al. | arXiv | 2016 | [1606.05250] |
| StarCoder 2 and The Stack v2: The Next Generation | Lozhkov et al. | arXiv | 2024 | [2402.19173] |

## Time-Series Foundationnal Models

| Title |  Author(s) | Journal | Year | DOI  |  
|-------|------------|---------|------|------|
| Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting | Lim et al. | arXiv | 2020 | [1912.09363] |
| N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting | Challu et al. | arXiv | 2022 | [2201.12886] |

## Diffusion Models


| Title |  Author(s) | Journal | Year | DOI  |  
|-------|------------|---------|------|------|
| Denoising Diffusion Probabilistic Models | Ho et al. | NeurIPS | 2020 | [2006.11239] |
| Generative Diffusion Models on Graphs: Methods and Applications | Liu et al. | arXiv | 2023 | [2302.02591] |


## EB-Models Articles

| Title |  Author(s) | Journal | Year | DOI  |  
|-------|------------|---------|------|------|
| A tutorial on Energy-Based Learning | LeCun et al. | MIT Press | 2006 | [eb-learning] |
| Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One | Grathwohl et al. | arXiv | 2019 | [1912.03263] |
| How to Train Your Energy-Based Models | Song et al. | arXiv | 2021 | [2101.03288] |
| HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly | Yen et al. | arXiv | 2024 | [2410.02694] |
| Energy-Based Transformers are Scalable Learners and Thinkers | Gladstone et al. | arXiv | 2025 | [2507.02092] |


## Tsetlin Machines Articles

| Title |  Author(s) | Journal | Year | DOI  |  
|-------|------------|---------|------|------|
| The Tsetlin Machine -- A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic | Ole-Christoffer Granmo | arXiv | 2018 | [1804.01508] | 
| Label-Critic Tsetlin Machine: A Novel Self-supervised Learning Scheme for Interpretable Clustering | Abouzeid et al. | IEEE | 2022 | [10.1109/ISTM54910.2022.00016]


[10.1109/ISTM54910.2022.00016]: https://ieeexplore.ieee.org/document/9923796
[1606.05250]: https://arxiv.org/abs/1606.05250
[1706.03762]: https://arxiv.org/abs/1706.03762
[1804.01508]: https://arxiv.org/abs/1804.01508
[1810.04805]: https://arxiv.org/abs/1810.04805
[1912.03263]: https://arxiv.org/abs/1912.03263
[1912.09363]: https://arxiv.org/abs/1912.09363
[2005.14165]: https://arxiv.org/abs/2005.14165
[2006.11239]: https://arxiv.org/abs/2006.11239
[2006.16236]: https://arxiv.org/abs/2006.16236
[2009.06732]: https://arxiv.org/abs/2009.06732
[2009.06732]: https://arxiv.org/abs/2009.06732
[2101.03288]: https://arxiv.org/abs/2101.03288
[2101.03961]: https://arxiv.org/abs/2101.03961
[2104.09864]: https://arxiv.org/abs/2104.09864
[2106.09685]: https://arxiv.org/abs/2106.09685
[2201.12886]: https://arxiv.org/abs/2201.12886
[2203.15556]: https://arxiv.org/abs/2203.15556
[2203.15556]: https://arxiv.org/abs/2203.15556
[2204.02311]: https://arxiv.org/abs/2204.02311
[2205.14135]: https://arxiv.org/pdf/2205.14135
[2302.02591]: https://arxiv.org/abs/2302.02591
[2302.02591]: https://arxiv.org/abs/2302.02591
[2305.14314]: https://arxiv.org/abs/2305.14314
[2307.08691]: https://arxiv.org/abs/2307.08691
[2309.00071]: https://arxiv.org/abs/2309.00071
[2309.16039]: https://arxiv.org/abs/2309.16039
[2310.06825]: https://arxiv.org/abs/2310.06825
[2312.00752]: https://arxiv.org/abs/2312.00752
[2402.19173]: https://arxiv.org/abs/2402.19173
[2410.02660]: https://arxiv.org/pdf/2410.02660
[2410.02694]: https://arxiv.org/abs/2410.02694
[2411.15242]: https://arxiv.org/abs/2411.15242
[2502.16982]: https://arxiv.org/abs/2502.16982
[2507.02092]: https://arxiv.org/abs/2507.02092
[2507.20534]: https://arxiv.org/abs/2507.20534
[unsupervised-multitask]: https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf
[eb-learning]: [unknown](https://www.researchgate.net/publication/200744586_A_tutorial_on_energy-based_learning)

